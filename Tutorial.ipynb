{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to guide the user through use of the src package. The package is intended to be run in the command line, without modification of intermediate output. Some of the script-generated objects (such as the trained emulators and MCMC chains) can be accessed in other Python scripts or Jupyter notebooks - the details are below. Often, the most user input will come in specifying objects in the \\__init\\__ file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder of the document will provide the bash commands to execute the scripts of src. Note that for bash commands to be run in Jupyter Notebook, they must be proceeded by \"!\" - when running the commands in Terminal, leave out the exclamation point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - \\__init\\__ file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the work for the user comes in setting up the \\__init\\__.py file. This script is called in all other scripts, and this is where the user inputs his or her own data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script need not be called by the user, but it must be edited by the user. Listed below are the objects which must be changed along with a short description, though the user is encouraged to read the documentation for specifics.\n",
    "\n",
    "* _systems_ - List of strings containing the collision systems involved. See the documentation for specifics\n",
    "* _keys_ - List of strings containing the input parameters\n",
    "* _labels_ - List of strings containing the LaTeX labels for the input parameters\n",
    "* _ranges_ - List of tuples containing the minimum and maximum for each input parameter\n",
    "* *design_array* - numpy array containing the design. If default of None remains unchanged, the code will generate a design from a Latin Hypercube\n",
    "* *data_list* - Dictionary of computer model output. Should be of the form \\[collision_system\\]\\[observable\\]\\[subobservable\\]\\['Y':, 'x':,\\]. This __must__ be changed from None.\n",
    "  * 'Y' is the 2D numpy array of output with rows corresponding to the rows of design_array\n",
    "  * 'x' is the 1D numpy array of indexing values of the columns of 'Y'\n",
    "* *exp_data_list* - Dictionary of experimental data. Should be of the form \\[collision_system\\]\\[observable\\]\\[subobservable\\]\\[{'y':,'x:',yerr: {'stat':,'sys':}}\\].  Thus __must__ be changed from None.\n",
    "  * 'y' is the 1D numpy array of experimental output \n",
    "  * 'x' is the 1D numpy array of indexing values of the columns of 'y'\n",
    "  * 'yerr' is a dictionary with keys 'stat' and 'sys'\n",
    "  * 'stat' is a 1D numpy array of statistical errors of experimental data\n",
    "  * 'sys' is a 1D numpy array of systematic errors of experimental data.\n",
    "  * This must be changed from None.\n",
    "* *exp_cov* - 2D numpy array of experimental covariance matrix. Recommended specified by user.\n",
    "  * If left unspecified (default), it will be calculated in _mcmc.py_ as follows:\n",
    "    * Each block between observables will be independent (0 matrix)\n",
    "    * Within an observable, blocks of subobservables will be indepdendent unless specified in _mcmc.py_\n",
    "    * Within a subobservable, the diagonal will be the sum of statistical and systematic error, and the $ij$th off-diagonal will be covariance calculated from distance between $x_i$ and $x_j$ using a squared-exponential covariance function. \n",
    "* *observables* - List of 2-tuples containing observable/subobservable pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Emulators - the emulator module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With design and data specified in the \\__init\\__ file, training the emulators is very simple. Simply run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][emulator] training emulator for system PbPb5020 (10 PC, 0 restarts)\n",
      "[0.29 0.3 ]\n",
      "Emulator design:\n",
      "[[0.01       0.05      ]\n",
      " [0.16058499 0.08485314]\n",
      " [0.30073974 0.13216592]\n",
      " [0.04547517 0.29132764]\n",
      " [0.15654149 0.19207646]\n",
      " [0.06263697 0.3387965 ]\n",
      " [0.1226533  0.14429201]\n",
      " [0.19871829 0.29998946]\n",
      " [0.07360878 0.10498264]\n",
      " [0.119071   0.27228267]\n",
      " [0.02518607 0.21059398]\n",
      " [0.24523197 0.24578168]\n",
      " [0.05462338 0.18687813]\n",
      " [0.01564432 0.25668644]\n",
      " [0.09639561 0.35834248]\n",
      " [0.01073974 0.15849388]\n",
      " [0.07697239 0.23242124]\n",
      " [0.04220857 0.11574906]\n",
      " [0.09144587 0.05834248]\n",
      " [0.03163994 0.07155832]\n",
      " [0.02060277 0.31787042]\n",
      " [0.3        0.35      ]\n",
      " [0.01       0.35      ]\n",
      " [0.3        0.05      ]]\n",
      "[INFO][emulator] writing cache file cache/emulator/PbPb5020.pkl\n",
      "PbPb5020\n",
      "10 PCs explain 0.99998 of variance\n",
      "GP 0: 0.99206 of variance, LML = 4.7553, kernel: 6.07**2 * RBF(length_scale=[0.264, 0.646]) + WhiteKernel(noise_level=0.00345)\n",
      "GP 1: 0.00645 of variance, LML = -22.392, kernel: 2.25**2 * RBF(length_scale=[0.0987, 0.22]) + WhiteKernel(noise_level=0.0583)\n",
      "GP 2: 0.00048 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[2.88, 3]) + WhiteKernel(noise_level=0.958)\n",
      "GP 3: 0.00034 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[1.63, 2.15]) + WhiteKernel(noise_level=0.958)\n",
      "GP 4: 0.00021 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[2.37, 3]) + WhiteKernel(noise_level=0.958)\n",
      "GP 5: 0.00020 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[1.55, 3]) + WhiteKernel(noise_level=0.958)\n",
      "GP 6: 0.00012 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[0.161, 0.03]) + WhiteKernel(noise_level=0.958)\n",
      "GP 7: 0.00006 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[2.84, 2.45]) + WhiteKernel(noise_level=0.958)\n",
      "GP 8: 0.00005 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[2.57, 2.97]) + WhiteKernel(noise_level=0.958)\n",
      "GP 9: 0.00003 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[0.888, 1.64]) + WhiteKernel(noise_level=0.958)\n"
     ]
    }
   ],
   "source": [
    "! python3 -m src.emulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the number of principal components from the default of 10, add the --npc flag. For example, to train the emulators on 3 components, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PbPb5020\r\n",
      "10 PCs explain 0.99998 of variance\r\n",
      "GP 0: 0.99206 of variance, LML = 4.7553, kernel: 6.07**2 * RBF(length_scale=[0.264, 0.646]) + WhiteKernel(noise_level=0.00345)\r\n",
      "GP 1: 0.00645 of variance, LML = -22.392, kernel: 2.25**2 * RBF(length_scale=[0.0987, 0.22]) + WhiteKernel(noise_level=0.0583)\r\n",
      "GP 2: 0.00048 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[2.88, 3]) + WhiteKernel(noise_level=0.958)\r\n",
      "GP 3: 0.00034 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[1.63, 2.15]) + WhiteKernel(noise_level=0.958)\r\n",
      "GP 4: 0.00021 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[2.37, 3]) + WhiteKernel(noise_level=0.958)\r\n",
      "GP 5: 0.00020 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[1.55, 3]) + WhiteKernel(noise_level=0.958)\r\n",
      "GP 6: 0.00012 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[0.161, 0.03]) + WhiteKernel(noise_level=0.958)\r\n",
      "GP 7: 0.00006 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[2.84, 2.45]) + WhiteKernel(noise_level=0.958)\r\n",
      "GP 8: 0.00005 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[2.57, 2.97]) + WhiteKernel(noise_level=0.958)\r\n",
      "GP 9: 0.00003 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[0.888, 1.64]) + WhiteKernel(noise_level=0.958)\r\n"
     ]
    }
   ],
   "source": [
    "! python3 -m src.emulator --npc 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user can also control number of restarts in the optimizer that estimates the GP hyperpameters. This is done with the --nrestarts flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important__: Once the emulators have been trained, they will be cached. After being cached, a call from the above lines will only print summaries to the console, and will not retrain the emulators. To retrain the emulators, either deleted the cached emulators or use the --retrain flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][emulator] training emulator for system PbPb5020 (10 PC, 0 restarts)\n",
      "[0.29 0.3 ]\n",
      "Emulator design:\n",
      "[[0.01       0.05      ]\n",
      " [0.16058499 0.08485314]\n",
      " [0.30073974 0.13216592]\n",
      " [0.04547517 0.29132764]\n",
      " [0.15654149 0.19207646]\n",
      " [0.06263697 0.3387965 ]\n",
      " [0.1226533  0.14429201]\n",
      " [0.19871829 0.29998946]\n",
      " [0.07360878 0.10498264]\n",
      " [0.119071   0.27228267]\n",
      " [0.02518607 0.21059398]\n",
      " [0.24523197 0.24578168]\n",
      " [0.05462338 0.18687813]\n",
      " [0.01564432 0.25668644]\n",
      " [0.09639561 0.35834248]\n",
      " [0.01073974 0.15849388]\n",
      " [0.07697239 0.23242124]\n",
      " [0.04220857 0.11574906]\n",
      " [0.09144587 0.05834248]\n",
      " [0.03163994 0.07155832]\n",
      " [0.02060277 0.31787042]\n",
      " [0.3        0.35      ]\n",
      " [0.01       0.35      ]\n",
      " [0.3        0.05      ]]\n",
      "[INFO][emulator] writing cache file cache/emulator/PbPb5020.pkl\n",
      "PbPb5020\n",
      "10 PCs explain 0.99998 of variance\n",
      "GP 0: 0.99206 of variance, LML = 4.7553, kernel: 6.07**2 * RBF(length_scale=[0.264, 0.646]) + WhiteKernel(noise_level=0.00345)\n",
      "GP 1: 0.00645 of variance, LML = -22.392, kernel: 2.25**2 * RBF(length_scale=[0.0987, 0.22]) + WhiteKernel(noise_level=0.0583)\n",
      "GP 2: 0.00048 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[2.88, 3]) + WhiteKernel(noise_level=0.958)\n",
      "GP 3: 0.00034 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[1.63, 2.15]) + WhiteKernel(noise_level=0.958)\n",
      "GP 4: 0.00021 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[2.37, 3]) + WhiteKernel(noise_level=0.958)\n",
      "GP 5: 0.00020 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[1.55, 3]) + WhiteKernel(noise_level=0.958)\n",
      "GP 6: 0.00012 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[0.161, 0.03]) + WhiteKernel(noise_level=0.958)\n",
      "GP 7: 0.00006 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[2.84, 2.45]) + WhiteKernel(noise_level=0.958)\n",
      "GP 8: 0.00005 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[2.57, 2.97]) + WhiteKernel(noise_level=0.958)\n",
      "GP 9: 0.00003 of variance, LML = -33.544, kernel: 0.00316**2 * RBF(length_scale=[0.888, 1.64]) + WhiteKernel(noise_level=0.958)\n"
     ]
    }
   ],
   "source": [
    "! python3 -m src.emulator --retrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing trained emulators in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user may wish to access the emulator outside of the scripts in this tutorial. Practical reasons include prediction or sampling for validation methods, or use in a more specialized analysis. To access the cached emulator with system 'PbPb5020' (for example), run the following lines in a python environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][emulator] training emulator for system PbPb5020 (10 PC, 0 restarts)\n",
      "[0.29 0.3 ]\n",
      "Emulator design:\n",
      "[[0.01       0.05      ]\n",
      " [0.16058499 0.08485314]\n",
      " [0.30073974 0.13216592]\n",
      " [0.04547517 0.29132764]\n",
      " [0.15654149 0.19207646]\n",
      " [0.06263697 0.3387965 ]\n",
      " [0.1226533  0.14429201]\n",
      " [0.19871829 0.29998946]\n",
      " [0.07360878 0.10498264]\n",
      " [0.119071   0.27228267]\n",
      " [0.02518607 0.21059398]\n",
      " [0.24523197 0.24578168]\n",
      " [0.05462338 0.18687813]\n",
      " [0.01564432 0.25668644]\n",
      " [0.09639561 0.35834248]\n",
      " [0.01073974 0.15849388]\n",
      " [0.07697239 0.23242124]\n",
      " [0.04220857 0.11574906]\n",
      " [0.09144587 0.05834248]\n",
      " [0.03163994 0.07155832]\n",
      " [0.02060277 0.31787042]\n",
      " [0.3        0.35      ]\n",
      " [0.01       0.35      ]\n",
      " [0.3        0.05      ]]\n"
     ]
    }
   ],
   "source": [
    "from src import emulator\n",
    "em = emulator.Emulator('PbPb5020')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object _em_ will now have all functionality of the Emulator class, trained on the data specified in the \\__init\\__ file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Calibration - the mcmc module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This modules runs the MCMC scheme to calibrate the inputs to the experimental data, using the trained emulators as a statistical surrogate for the expensive computer model. The script calls the python distribution emcee, which runs an affine-invariant sampler. The sampler requires a number of \"walkers\" which each run a chain in parallel, as well as a number of iterations to run each walker. A number of burn-in steps to discarded must also be specified (these allow the samplers to \"warm up\"). To run this module with 500 walkers, 200 burn-in steps, and 300 post-burn-in steps, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wk42/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "[INFO][mcmc] no existing chain found, starting initial burn-in\n",
      "[INFO][mcmc] running 500 walkers for 100 steps\n",
      "[INFO][mcmc] step 10: acceptance fraction: mean 0.4218, std 0.1807, min 0.0000, max 0.9000\n",
      "[INFO][mcmc] step 20: acceptance fraction: mean 0.4330, std 0.1345, min 0.0500, max 0.8000\n",
      "[INFO][mcmc] step 30: acceptance fraction: mean 0.4487, std 0.1102, min 0.1000, max 0.7667\n",
      "[INFO][mcmc] step 40: acceptance fraction: mean 0.4632, std 0.1003, min 0.1750, max 0.7500\n",
      "[INFO][mcmc] step 50: acceptance fraction: mean 0.4759, std 0.0956, min 0.1800, max 0.7200\n",
      "[INFO][mcmc] step 60: acceptance fraction: mean 0.4889, std 0.0913, min 0.2000, max 0.7500\n",
      "[INFO][mcmc] step 70: acceptance fraction: mean 0.5011, std 0.0889, min 0.2143, max 0.7571\n",
      "[INFO][mcmc] step 80: acceptance fraction: mean 0.5119, std 0.0863, min 0.2500, max 0.7625\n",
      "[INFO][mcmc] step 90: acceptance fraction: mean 0.5221, std 0.0821, min 0.2444, max 0.7556\n",
      "[INFO][mcmc] step 100: acceptance fraction: mean 0.5319, std 0.0780, min 0.2400, max 0.7400\n",
      "[INFO][mcmc] resampling walker positions\n",
      "[INFO][mcmc] running 500 walkers for 100 steps\n",
      "[INFO][mcmc] step 10: acceptance fraction: mean 0.7192, std 0.1951, min 0.1000, max 1.0000\n",
      "[INFO][mcmc] step 20: acceptance fraction: mean 0.6996, std 0.1518, min 0.0500, max 1.0000\n",
      "[INFO][mcmc] step 30: acceptance fraction: mean 0.6890, std 0.1296, min 0.2333, max 1.0000\n",
      "[INFO][mcmc] step 40: acceptance fraction: mean 0.6794, std 0.1135, min 0.3000, max 0.9500\n",
      "[INFO][mcmc] step 50: acceptance fraction: mean 0.6732, std 0.1021, min 0.3400, max 0.9200\n",
      "[INFO][mcmc] step 60: acceptance fraction: mean 0.6720, std 0.0938, min 0.3500, max 0.9167\n",
      "[INFO][mcmc] step 70: acceptance fraction: mean 0.6691, std 0.0876, min 0.3714, max 0.8714\n",
      "[INFO][mcmc] step 80: acceptance fraction: mean 0.6661, std 0.0819, min 0.4125, max 0.8750\n",
      "[INFO][mcmc] step 90: acceptance fraction: mean 0.6658, std 0.0768, min 0.4000, max 0.8556\n",
      "[INFO][mcmc] step 100: acceptance fraction: mean 0.6662, std 0.0724, min 0.3800, max 0.8400\n",
      "[INFO][mcmc] burn-in complete, starting production\n",
      "[INFO][mcmc] running 500 walkers for 300 steps\n",
      "[INFO][mcmc] step 30: acceptance fraction: mean 0.6463, std 0.1197, min 0.2333, max 0.9333\n",
      "[INFO][mcmc] step 60: acceptance fraction: mean 0.6504, std 0.0900, min 0.3500, max 0.9167\n",
      "[INFO][mcmc] step 90: acceptance fraction: mean 0.6479, std 0.0771, min 0.4000, max 0.8444\n",
      "[INFO][mcmc] step 120: acceptance fraction: mean 0.6498, std 0.0683, min 0.4333, max 0.8167\n",
      "[INFO][mcmc] step 150: acceptance fraction: mean 0.6505, std 0.0616, min 0.4200, max 0.8067\n",
      "[INFO][mcmc] step 180: acceptance fraction: mean 0.6514, std 0.0560, min 0.4389, max 0.8111\n",
      "[INFO][mcmc] step 210: acceptance fraction: mean 0.6515, std 0.0524, min 0.4429, max 0.8095\n",
      "[INFO][mcmc] step 240: acceptance fraction: mean 0.6524, std 0.0493, min 0.4458, max 0.7792\n",
      "[INFO][mcmc] step 270: acceptance fraction: mean 0.6523, std 0.0465, min 0.4778, max 0.7778\n",
      "[INFO][mcmc] step 300: acceptance fraction: mean 0.6518, std 0.0443, min 0.4833, max 0.7633\n",
      "[INFO][mcmc] writing chain to file\n"
     ]
    }
   ],
   "source": [
    "! python3 -m src.mcmc --nwalkers 500 --nburnsteps 200 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run an additional (for example) 100 steps with the same walkers, simply remove the --nwalkers and --nburnsteps flags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wk42/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "[INFO][mcmc] restarting from last point of existing chain\n",
      "[INFO][mcmc] running 500 walkers for 100 steps\n",
      "[INFO][mcmc] step 10: acceptance fraction: mean 0.6476, std 0.1863, min 0.1000, max 1.0000\n",
      "[INFO][mcmc] step 20: acceptance fraction: mean 0.6499, std 0.1415, min 0.1500, max 1.0000\n",
      "[INFO][mcmc] step 30: acceptance fraction: mean 0.6524, std 0.1150, min 0.3000, max 0.9000\n",
      "[INFO][mcmc] step 40: acceptance fraction: mean 0.6542, std 0.1016, min 0.3250, max 0.8750\n",
      "[INFO][mcmc] step 50: acceptance fraction: mean 0.6564, std 0.0935, min 0.3800, max 0.8600\n",
      "[INFO][mcmc] step 60: acceptance fraction: mean 0.6554, std 0.0862, min 0.3667, max 0.8667\n",
      "[INFO][mcmc] step 70: acceptance fraction: mean 0.6531, std 0.0798, min 0.3571, max 0.8429\n",
      "[INFO][mcmc] step 80: acceptance fraction: mean 0.6523, std 0.0756, min 0.3500, max 0.8375\n",
      "[INFO][mcmc] step 90: acceptance fraction: mean 0.6542, std 0.0713, min 0.3333, max 0.8333\n",
      "[INFO][mcmc] step 100: acceptance fraction: mean 0.6545, std 0.0690, min 0.3800, max 0.8200\n",
      "[INFO][mcmc] writing chain to file\n"
     ]
    }
   ],
   "source": [
    "! python3 -m src.mcmc 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: To restart the chain, you must delete the chain.hdf file in the _mcmc/_ directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing posterior samples in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user may wish to access the posterior samples outside of the scripts in this tutorial. Practical reasons include making specific plots, or for getting posterior estimates of functions of the parameters. To access a saved chain, run the following commands in a python environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import mcmc\n",
    "chain = mcmc.Chain()\n",
    "posterior_samples = chain.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object _chain_ is an instance of the Chain class, while *posterior_samples* is a 2D numpy array where each row is a draw from the joint posterior distribution. For marginal posteriors, simply use single columns of *posterior_samples*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Results - the plots module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This package contains some basic plotting tools to visualize different aspects of the analysis. Plots are saved in the _plot_ directory. To create a plot, simply the add it as a positional argument. For example, to make the \"posterior\" plot, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wk42/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "[INFO][plots] generating plot: posterior\n",
      "/home/wk42/.local/lib/python3.6/site-packages/matplotlib/font_manager.py:1316: UserWarning: findfont: Font family ['cursive'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n",
      "[INFO][plots] wrote plots/posterior.pdf\n"
     ]
    }
   ],
   "source": [
    "! python3 -m src.plots posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the names of plots available for plotting, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wk42/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "usage: plots.py [-h] [PLOT [PLOT ...]]\n",
      "\n",
      "generate plots\n",
      "\n",
      "positional arguments:\n",
      "  PLOT        {observables_design, observables_posterior, posterior, design,\n",
      "              gp, diag_emu} (default: all)\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help  show this help message and exit\n"
     ]
    }
   ],
   "source": [
    "! python3 -m src.plots --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The available plots are described below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __observable_design__\n",
    "    * Model observables at design points, with experimental data plotted as reference.\n",
    "    * __IMPORTANT__: For different observables than the example, change the dictionary in \\_observables_plot()\n",
    "* __observable_posterior__\n",
    "    * Model observables at 100 draws from the posterior, with experimental data plotted as reference.\n",
    "    * __IMPORTANT__: For different observables than the example, change the dictionary in \\_observables_plot()\n",
    "* __posterior__\n",
    "    * Pairplot of posteriors for all calibration inputs. Diagonal displays marginal density.Lower off-diagonal displays pairwise scatter plot.\n",
    "* __design__\n",
    "    * Projection of a LH design into two dimensions. Change keys within the function to the two inputs you want to protect into.\n",
    "* __gp__\n",
    "    * Conditioning a Gaussian process. Simple example plots with dummy data.\n",
    "* __diag_emu__\n",
    "    * Diagnostic: plots of each principal component vs each input parameter, overlaid by emulator predictions at several points in design space.\n",
    "    * See how well the emulators track the design points, if uncertainty and shape of predictions are reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: The user may observe that _plots.py_ contains additional plotting functions. Some of these are helper functions, but many are copied over from the original distribution from which this distribution is forked. Most of the unreported functions are hard-coded for that project's observables, so support for those functions were excluded in this package."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
