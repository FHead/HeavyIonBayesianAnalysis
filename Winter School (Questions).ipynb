{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n",
    "from sklearn.gaussian_process import kernels\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats\n",
    "import emcee\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Latin Hypercube Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lhs(npoints, ndim, seed):\n",
    "    \"\"\"\n",
    "    Generate a maximin Latin-hypercube sample (LHS) with the given number of\n",
    "    points, dimensions, and random seed.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    proc = subprocess.run(\n",
    "        ['R', '--slave'],\n",
    "        input=\"\"\"\n",
    "        library('lhs')\n",
    "        set.seed({})\n",
    "        write.table(maximinLHS({}, {}), col.names=FALSE, row.names=FALSE)\n",
    "        \"\"\".format(seed, npoints, ndim).encode(),\n",
    "        stdout=subprocess.PIPE,\n",
    "        check=True\n",
    "    )\n",
    "\n",
    "    lhs = np.array(\n",
    "        [l.split() for l in proc.stdout.splitlines()],\n",
    "        dtype=float\n",
    "    )\n",
    "\n",
    "\n",
    "    return lhs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and plot design matrix below\n",
    "##### 20 points, 2 dimensions, set the seed to 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "design = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x = , y = )\n",
    "plt.title('Latin Hypercube Design')\n",
    "plt.xlabel('Input 1')\n",
    "plt.ylabel('Input 2')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Toy GP Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a - Mean and Variance Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truth(x):\n",
    "    return(3*x+np.cos(5*x))    \n",
    "design = np.linspace(start =-1,stop=1,num=5)\n",
    "model_data = truth(design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=design,y=model_data)\n",
    "plt.title('Computer Model Output at Design Points')\n",
    "plt.xlabel('Design')\n",
    "plt.ylabel('Model Output')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptp = 2\n",
    "kernel = (\n",
    "    1. * kernels.RBF(\n",
    "        length_scale=ptp,\n",
    "        length_scale_bounds=np.outer(ptp, (.1, 10))\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = GPR(kernel=kernel,\n",
    "    #alpha=0,\n",
    "    n_restarts_optimizer=0,\n",
    "    copy_X_train=False).fit(design.reshape(-1,1), model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, cov = gp.predict(return_cov=True,X=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the figure by first plotting the output at the design points\n",
    "plt.scatter(x=design,y=model_data,color = 'black',label = 'Design Output')\n",
    "plt.title('Computer Model Output at Design Points')\n",
    "plt.xlabel('Design')\n",
    "plt.ylabel('Model Output')\n",
    "\n",
    "\n",
    "#Add the mean, upper 95% quantile, and lower 95% quantile of the GP predictions at all the in-between points\n",
    "plt.plot(x=X, y = ,color= 'blue',label = 'GP Mean')\n",
    "top_var =\n",
    "bot_var = \n",
    "plt.fill_between(X[:,0], bot_var, top_var, where=top_var >= bot_var, facecolor='lightgray', interpolate=True)\n",
    "\n",
    "plt.plot(X,truth(X),color='black',label = 'Truth')\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part b - Random Draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get the upper 95% quantile, and lower 95% quantile of the GP predictions at all the in-between points\n",
    "#Same as before\n",
    "top_var = \n",
    "bot_var = \n",
    "\n",
    "plt.scatter(x=design,y=model_data,color = 'black',label = 'Design Output')\n",
    "plt.title('Computer Model Output at Design Points')\n",
    "plt.xlabel('Design')\n",
    "plt.ylabel('Model Output')\n",
    "plt.fill_between(X[:,0], bot_var, top_var, where=top_var >= bot_var, facecolor='lightgray', interpolate=True)\n",
    "plt.plot(X,truth(X),color='black',label = 'Truth')\n",
    "\n",
    "ndraws = 10\n",
    "colors = cm.rainbow(np.linspace(0, 1, ndraws))\n",
    "\n",
    "#Get [ndraws] random draws from the predictive distribution of the GP at all of the in-between points \n",
    "rand_draw = \n",
    "for i in range(ndraws):\n",
    "    plt.plot(X,rand_draw[:,i],color = colors[i],linestyle = \":\")\n",
    "\n",
    "plt.legend(loc='best', fontsize=14)\n",
    "\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = np.loadtxt('scores.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(copy=False)\n",
    "pca = PCA(copy=False, whiten=True, svd_solver='full')\n",
    "Z = pca.fit_transform(scaler.fit_transform(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cummulative fraction of variance explained. How many PCs would you recommend using?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F_r = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(F_r)),F_r,'-o')\n",
    "plt.title('Fraction of Variance Explained')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('F_r')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the correlation between the first two principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = \n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the second principal component against the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter()\n",
    "plt.title('Principle Components')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Bayes Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('unknown_mean.txt',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "known_var = 5\n",
    "data = np.loadtxt('unknown_mean.txt')\n",
    "plt.hist(data)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the data are Normally distributed, with known variance $\\sigma_y^2 = 5$ but unkown mean $\\theta$. We place a Normal prior on $\\theta$, with mean $\\mu$ and variance $\\tau$. Thus we have the following setup:\n",
    "\n",
    "Likelihood:\n",
    "\\begin{align}\n",
    "    \ty_i\\mid\\theta&\\overset{iid}{\\sim} N(\\theta,\\sigma_y^2)\\quad\\text{for}\\ \\ i\\in{1,\\ldots,n}\n",
    "\\end{align}\n",
    "Prior:\n",
    "\\begin{align}\n",
    "\t\\theta &\\sim N(\\mu,\\tau)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Explore different priors for $\\theta$ to see how the posterior of $\\theta$ responds. Try combinations of the following values:\n",
    "\n",
    "$\\mu\\in \\{0,3,10\\}$\n",
    "\n",
    "$\\tau\\in \\{1,10,0.1\\}$\n",
    "\n",
    "Report the mean and variance for the posterior of each pair, and plot histograms of 10,000 draws from the posterior for 2-3 pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: MCMC via python package emcee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in missing pieces to calc_lnprior and calc_lnlike. These functions calculate the log of the prior and likelihood, respectively. For example, for the prior, you need to calculate the log of the pdf at theta for given mean mu and variance tau.\n",
    "\n",
    "Hint 1: The liklihood of all data points is the product over all $n$ individual likelihoods. What is the log likelihood of all data points?\n",
    "\n",
    "\n",
    "Hint 2: If done correctly:\n",
    "\n",
    "calc_lnlike(theta=2, mu = 0, tau = 1) = -2.919\n",
    "\n",
    "calc_lnlike(y=data,theta=2,sigma_y_sqrd=5)=-157.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the log of the prior of theta\n",
    "def calc_lnprior(theta,mu,tau):\n",
    "    #Put log of proper normal pdf here\n",
    "    return \n",
    "\n",
    "#Calculate the log of the likelihood of {y_1, y_2,...,y_n}\n",
    "def calc_lnlike(theta, y,sigma_y_sqrd):\n",
    "    #Put log of proper normal likelihood pdf in here\n",
    "    return \n",
    "\n",
    "def lnposterior(theta,mu,tau,y,sigma_y_sqrd):\n",
    "    ln_pr = calc_lnprior(theta,mu,tau)\n",
    "    \n",
    "    if not np.isfinite(ln_pr):\n",
    "        return -np.inf\n",
    "        \n",
    "    ln_like = calc_lnlike(theta=theta, y=y, sigma_y_sqrd=sigma_y_sqrd)\n",
    "    return ln_pr + ln_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below sets up the sampler. We provide the number of walkers nwalkers (chosen), the number of parameters ndim (1 in our case, since we just have $\\theta$), and the posterior function which takes as argument the parameters we're inferrin on. \"args\" is a tuple of all of arguments to the probability function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ndim, nwalkers = 1, 200\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnposterior, args=(mu, tau, data,known_var))\n",
    "p0 = np.random.rand(nwalkers,ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$nsteps$ is the number of steps each walker will run, while $nburnin$ is the number of \"burn-in\" or \"warmup\" steps for each walker.  Our total number of samples will be $nwalkers\\times (nsteps-nburnin)$ (recall we want 10,000 total samples). After running each walker for $nsteps$ and discarding the first $nburnin$, we reshape the output chain so each row is a draw from the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 100\n",
    "nburnin= 50\n",
    "out_post = sampler.run_mcmc(p0,nsamples)\n",
    "samples = sampler.chain[:, nburnin:, :].reshape((-1, ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the samples to find posterior means and variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(samples)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Direct computation of the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the analytics posterior in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, recall the likelihood and prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\ty_i\\mid\\theta&\\overset{iid}{\\sim} N(\\theta,\\sigma_y^2)\\\\\n",
    "\t\\theta &\\sim N(\\mu,\\tau)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we apply Bayes Rule and do a little algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\tp(\\theta\\mid\\mathbf{y}) &\\propto p(\\mathbf{y}\\mid\\theta)p(\\theta)\\\\\n",
    "        \t\t&\\propto \\left[\\prod\\limits_{i=1}^np(y_i\\mid\\theta)\\right]p(\\theta)\\\\\n",
    "               \t&\\propto \\left[\\prod\\limits_{i=1}^n(2\\pi\\sigma_y^2)^{-1/2}e^{-\\frac{1}{2\\sigma_y^2}(y_i - \\theta)^2}\\right](2\\pi\\tau)^{-1/2}e^{-\\frac{1}{2\\tau}(\\theta-\\mu)}\\\\\n",
    "\t&\\vdots\\\\\n",
    "\t\\theta\\mid\\mathbf{y}&\\sim N\\left(\\left(\\frac{\\bar{y}}{\\sigma_y^2}+\\frac{\\mu}{\\tau}\\right)\\left(\\frac{1}{\\sigma_y^2} + \\frac{1}{\\tau}\\right)^{-1},\\left(\\frac{1}{\\sigma_y^2} + \\frac{1}{\\tau}\\right)^{-1} \\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the code below to calculate the posterior mean and variance. Compare to above for the two pairs for which you plotted the posterior histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mu = \n",
    "tau = \n",
    "post_var = \n",
    "print(post_var)\n",
    "post_mean = \n",
    "print(post_mean)\n",
    "post_draws = np.random.normal(post_mean,np.sqrt(post_var),10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
