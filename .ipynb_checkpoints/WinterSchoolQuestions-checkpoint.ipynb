{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n",
    "from sklearn.gaussian_process import kernels\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "import emcee\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Bayes Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're going to explore the mechanics of Bayesian analysis, and how prior choices can affect the posterior outcome. Let's revisit the coin example from the slides - we flipped 10 coins, and 7 came up heads. Our goal to make inference on the true probability of heads $\\theta$, using Bayesian analysis. To do this, we need a likelihood and prior. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_tosses = np.loadtxt('coin_tosses.txt')\n",
    "num_heads = sum(coin_tosses)\n",
    "N = len(coin_tosses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Likelihood**:\n",
    "A common model for independent coin flips (or anything with a 0/1 outcome) is Binomial. So our likelihood is Binomial with size N = 10 and probability $\\theta$:\n",
    "\\begin{align}\n",
    "    y\\sim \\text{Binom}(N,\\theta)\n",
    "\\end{align}\n",
    "\n",
    "The mean of a Binomial random variable $Y$ with size $N$ (flips) and probability $\\theta$ (probability of heads) is\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathsf{E}(Y) = N\\theta.\n",
    "\\end{align}\n",
    "\n",
    "The variance of $Y$ is\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathsf{V}(Y) = N\\theta(1-\\theta)\n",
    "\\end{align}\n",
    "\n",
    "More information on Binomial distribution can found here: https://en.wikipedia.org/wiki/Binomial_distribution. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prior**:\n",
    "The most widely used prior for Binomial data and unknown probability is a Beta distribution. A Beta random variable can take value in (0,1).  \n",
    "\n",
    "\\begin{align}\n",
    "    \\theta\\sim \\text{Beta}(a, b)\n",
    "\\end{align}\n",
    "\n",
    "The mean of a Beta random variable $\\theta$ with first parameter $a$ and second paramter $b$ is\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathsf{E}(\\theta) = \\frac{a}{a+b}\n",
    "\\end{align}\n",
    "\n",
    "The variance of $\\theta$ is\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathsf{V}(\\theta) = \\frac{a + b}{(a+b)^2(a+b+1)}\n",
    "\\end{align}\n",
    "\n",
    "More information on the Beta distribution can found here: https://en.wikipedia.org/wiki/Beta_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Explore different priors for $\\theta$ to see how the posterior of $\\theta$ responds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: MCMC via python package emcee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in missing pieces to calc_lnprior and calc_lnlike. These functions calculate the log of the prior and likelihood, respectively. For example, for the prior, you need to calculate the log of the pdf at theta for given parameters $a$ and $b$. Use the Wikipedia pages to find the pdfs.\n",
    "\n",
    "    Hint 1: For the Beta(a,b) prior, you will need to calculate the Beta function of parameters a and b (see the Wikipedia page). The cell below imports betln, which calculates the natural log of the Beta function.\n",
    "\n",
    "    Hint 2: For the Binomial likelihood, you'll need to calculate \n",
    "$$\\left(\\begin{matrix}N\\\\y\\end{matrix}\\right)$$\n",
    "    \n",
    "    This is the number of ways to get y heads in N flips. The function nCr(N,y) in the cell below calculates this.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import betaln\n",
    "import math\n",
    "\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return f(n) // f(r) // f(n-r)\n",
    "\n",
    "print(nCr(4,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the log pdf of the prior of theta\n",
    "def prior_ln_pdf(theta,a,b):\n",
    "    #Ensure theta is within (0,1)\n",
    "    if theta <=0 or theta >=1:\n",
    "        return -np.inf\n",
    "    \n",
    "    ##Add return\n",
    "    return ??\n",
    "\n",
    "#Calculate the log pdf of the likelihood of y successes (heads) in N trials (flips)\n",
    "def likelihood_ln_pdf(y, theta, N):\n",
    "\n",
    "    ##Add function return\n",
    "    return ??\n",
    "\n",
    "def posterior_ln_pdf(theta,a,b,y, N):\n",
    "    ln_pr = prior_ln_pdf(theta=theta,a=a,b=b)\n",
    "    \n",
    "    if not np.isfinite(ln_pr):\n",
    "        return -np.inf\n",
    "        \n",
    "    ln_like = likelihood_ln_pdf(y=y, theta=theta,  N = N)\n",
    "    return ln_pr + ln_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Hint 3: If done correctly:\n",
    "\n",
    "    prior_ln_pdf(theta=0.4, a = 2,b = 3) = 0.547\n",
    "\n",
    "    likelihood_ln_pdf(theta=0.5,y=num_heads,N=10) = -2.144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Check functions\n",
    "print(np.round(prior_ln_pdf(theta=0.4, a = 2,b = 3),3))\n",
    "print(np.round(likelihood_ln_pdf(theta=0.5,y=num_heads,N=10),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below sets up the sampler. $nsteps$ is the number of steps each walker will run, while $nburnin$ is the number of \"burn-in\" or \"warmup\" steps for each walker.  Our total number of samples will be $nwalkers\\times (nsteps-nburnin)$ (recall we want 10,000 total samples). After running each walker for $nsteps$ and discarding the first $nburnin$, we reshape the output chain so each row is a draw from the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior_draws(a,b, nsamples = 100, nburnin = 50, nwalkers = 200):\n",
    "    ndim = 1\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, posterior_ln_pdf, args=(a, b, num_heads,N))\n",
    "    p0 = np.random.rand(nwalkers,ndim)\n",
    "    out_post = sampler.run_mcmc(p0,nsamples)\n",
    "    samples = sampler.chain[:, nburnin:, :].reshape((-1, ndim))\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the samples to find posterior means and variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare the prior density to the posterior samples\n",
    "def plot_comparison_mcmc(prior_a, prior_b, nsamples = 100,nburnin = 50, nwalkers = 200):\n",
    "    theta_grid = np.linspace(0.01,0.99,100)\n",
    "    \n",
    "    #Calculate the posterior density at the grid of theta values\n",
    "    prior_pdf_vals = stats.beta(prior_a,prior_b).pdf(theta_grid)\n",
    "    \n",
    "    samples = get_posterior_draws(a = prior_a, b = prior_b,\n",
    "                                  nsamples = nsamples, nburnin = nburnin, nwalkers = nwalkers)\n",
    "    \n",
    "    print('Prior mean is')\n",
    "    print(prior_a/(prior_b+prior_b))\n",
    "    print('Posterior mean is')\n",
    "    print(np.round(samples.mean(),2))\n",
    "    \n",
    "    plt.plot(theta_grid,prior_pdf_vals,label = 'Prior Density' )\n",
    "    plt.hist(samples,density = True,label = 'Posterior Samples',bins=20)\n",
    "    plt.legend(loc='best', fontsize=12)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: run $plot\\_comparison\\_mcmc()$ with the following choices of a and b to compare the prior density to the histogram of posterior samples. Also report the prior mean versus the mean of the posterior samples\n",
    "\n",
    "a = 1, b = 1\n",
    "\n",
    "a = 1, b = 10\n",
    "\n",
    "a = 10, b = 1\n",
    "\n",
    "a = 30, b = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison_mcmc(??, ??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Direct computation of the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the analytic posterior in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, recall the likelihood and prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\ty\\mid\\theta&\\sim \\mathsf{Binom}(N,\\theta)\\\\\n",
    "\t\\theta&\\sim \\mathsf{Beta}(a,b)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we apply Bayes Rule and do a little algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\tp(\\theta\\mid y)&\\propto p(y\\mid\\theta)p(\\theta)\\\\\n",
    "\t &= \\left[\\begin{pmatrix}N \\\\ y\\end{pmatrix}\\theta^y(1-\\theta)^{N-y}\\right]\\frac{\\theta^{a-1}(1-\\theta)^{b-1}}{B(a,b)}\\\\\n",
    "\t &\\propto \\theta^{a+y-1}(1-\\theta)^{b+N-y-1}\n",
    "\\end{align}\n",
    "Note that when doing this algebra, we can disregard the constants $\\begin{pmatrix}N \\\\ y\\end{pmatrix}$ and $B(a,b)$ because they fall away in the proportionality.\n",
    "\n",
    "At the end, we recognize the kernel of a $\\mathsf{Beta}(a^*,b^*)$ distribution, where\n",
    "\\begin{align}\n",
    "\ta^* &= a + y\\\\\n",
    "\tb^* &= b + N - y\n",
    "\\end{align}\n",
    "This is posterior distribution of $\\theta$.\n",
    "\n",
    "Note that this math was very simple only because of the choice of prior and likelihood; a Beta distribution is a so-called \"conjugate prior\" for a Binomial likelihood. For more comlicated models the posterior will not be available analytically, and we must resort to sampling methods. We only used sampling methods in this model for pedagogical purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_comparison_analytic(prior_a,prior_b, posterior_a, posterior_b,\n",
    "                            nsamples = 100,nburnin = 50, nwalkers = 200):\n",
    "\n",
    "    theta_grid = np.linspace(0.01,0.99,100)\n",
    "    \n",
    "    #Calculate the prior pdf at the grid of theta values\n",
    "    prior_pdf_vals = stats.beta(prior_a,prior_b).pdf(theta_grid)\n",
    "    plt.plot(theta_grid, prior_pdf_vals, label = 'Prior')\n",
    "    plt.xlabel('Theta')\n",
    "    plt.ylabel('Pdf')\n",
    "    \n",
    "    #Compare to MCMC-generated results\n",
    "    samples = get_posterior_draws(a = prior_a, b = prior_b, \n",
    "                                 nsamples = nsamples, nburnin = nburnin, nwalkers = nwalkers)\n",
    "    plt.hist(samples,density = True,label = 'Posterior Samples', bins = 20)\n",
    "    \n",
    "    #Calculate the posterior pdf at the grid of theta values\n",
    "    posterior_pdf_vals = stats.beta(posterior_a,posterior_b).pdf(theta_grid)\n",
    "    plt.plot(theta_grid,posterior_pdf_vals, label = 'Analytic Posterior')\n",
    "    \n",
    "    \n",
    "    print('Prior mean is')\n",
    "    print(prior_a/(prior_b + prior_b))\n",
    "    print('Analytic posterior mean is')\n",
    "    print(np.round(posterior_a/(posterior_a + posterior_b),2))\n",
    "    print('Posterior mean from MCMC is')\n",
    "    print(np.round(samples.mean(),2))\n",
    "    \n",
    "    \n",
    "    plt.legend(loc='best', fontsize=12)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Find the parameters $a\\_star$ and $b\\_star$ for the posterior distribution of $\\theta$ in our model. Compare the prior density to posterior density for the above choices of $a$ and $b$. Do the posterior densities align with the histogram of your posterior samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = #your choice\n",
    "b = #your choice\n",
    "N = len(coin_tosses)\n",
    "y = sum(coin_tosses)\n",
    "a_star = ??\n",
    "b_star = ??\n",
    "\n",
    "plot_comparison_analytic(prior_a = a, prior_b =  b,\n",
    "                         posterior_a = a_star, posterior_b =  b_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Latin Hypercube Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lhs(npoints, ndim, seed):\n",
    "    \"\"\"\n",
    "    Generate a maximin Latin-hypercube sample (LHS) with the given number of\n",
    "    points, dimensions, and random seed.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    proc = subprocess.run(\n",
    "        ['R', '--slave'],\n",
    "        input=\"\"\"\n",
    "        library('lhs')\n",
    "        set.seed({})\n",
    "        write.table(maximinLHS({}, {}), col.names=FALSE, row.names=FALSE)\n",
    "        \"\"\".format(seed, npoints, ndim).encode(),\n",
    "        stdout=subprocess.PIPE,\n",
    "        check=True\n",
    "    )\n",
    "\n",
    "    lhs = np.array(\n",
    "        [l.split() for l in proc.stdout.splitlines()],\n",
    "        dtype=float\n",
    "    )\n",
    "\n",
    "\n",
    "    return lhs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and plot design matrix below\n",
    "##### 20 points, 2 dimensions, set the seed to 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Fill in the parameters for generate_lhs(), and for plt.scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Fill in the parameters to generate_lhs\n",
    "design = generate_lhs(npoints = ??,\n",
    "                      ndim = ??,\n",
    "                      seed = ??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Fill in parameters to plt.scatter\n",
    "plt.scatter(x = ??,\n",
    "           y = ??) \n",
    "plt.title('Latin Hypercube Design')\n",
    "plt.xlabel('Input 1')\n",
    "plt.ylabel('Input 2')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Toy GP Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a - Mean and Variance Estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the two cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truth(x):\n",
    "    return(3*x+np.cos(5*x))    \n",
    "design = np.linspace(start =-1,stop=1,num=5)\n",
    "model_data = truth(design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=design,y=model_data)\n",
    "plt.title('Computer Model Output at Design Points')\n",
    "plt.xlabel('Design')\n",
    "plt.ylabel('Model Output')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below, training the GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptp = 2\n",
    "kernel = (\n",
    "    1. * kernels.RBF(\n",
    "        length_scale=ptp,\n",
    "        length_scale_bounds=np.outer(ptp, (.1, 10))\n",
    "    ) \n",
    ")\n",
    "gp = GPR(kernel=kernel,\n",
    "    #alpha=0,\n",
    "    n_restarts_optimizer=0,\n",
    "    copy_X_train=False).fit(design.reshape(-1,1), model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Create the vector of points X on which we will predict. X should be a (n x 1) numpy array (you choose n)\n",
    "\n",
    "    Hint: We only want to predict within the bounds of our design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ??\n",
    "X = X.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This returns the predictive mean and covariance at all the points in X\n",
    "#mean is a (n,) numpy array, and cov is a (n,n) numpy array\n",
    "mean, cov = gp.predict(return_cov=True,X=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Fill in the lines for plt.plot, top_var, and bot_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the figure by first plotting the output at the design points\n",
    "plt.scatter(x = design,y = model_data,color = 'black',label = 'Design Output')\n",
    "plt.title('Computer Model Output at Design Points')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Model Output')\n",
    "\n",
    "\n",
    "#Add the mean 95% uncertainty interval of the GP predictions at all the in-between points\n",
    "##Fill in parameters to plt.plot\n",
    "##Find expressions for top_var and bot_var\n",
    "###### Hint: For a normal distribution, how many standard deviations away from the mean is the 97.5% quantile?\n",
    "###### Hint: Use the diagonal of predictive covariance matrix`\n",
    "plt.plot(X ,mean ,color= 'blue',label = 'GP Mean')\n",
    "top_var = ??\n",
    "bot_var = ??\n",
    "plt.fill_between(X[:,0], bot_var, top_var, where=top_var >= bot_var, facecolor='lightgray', interpolate=True)\n",
    "\n",
    "plt.plot(X,truth(X),color='black',label = 'Truth')\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part b - Random Draws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior exercises displayed the mean and variance of our function at all the points in X. But what about actual samples of the function itself? Here we visualize what a random draw of the function would look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercse**: Fill in the lines for top_var, bot_var, and rand_draw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Hint: For rand_draw, examine np.random.multivariate_normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get the upper 95% quantile, and lower 95% quantile of the GP predictions at all the in-between points\n",
    "##Fill in same values as previous exercise\n",
    "top_var = ??\n",
    "bot_var = ??\n",
    "\n",
    "plt.scatter(design,model_data,color = 'black',label = 'Design Output')\n",
    "plt.title('Computer Model Output at Design Points')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Model Output')\n",
    "plt.fill_between(X[:,0], bot_var, top_var, where=top_var >= bot_var, facecolor='lightgray', interpolate=True)\n",
    "plt.plot(X,truth(X),color='black',label = 'Truth')\n",
    "\n",
    "ndraws = 10\n",
    "colors = cm.rainbow(np.linspace(0, 1, ndraws))\n",
    "\n",
    "#Get [ndraws] random draws from the predictive distribution of the GP at all of the in-between points \n",
    "#Use the predictive mean and covariance\n",
    "#Hint: examine np.random.multivariate_normal()\n",
    "rand_draw = ??\n",
    "for i in range(ndraws):\n",
    "    plt.plot(X,rand_draw[i,:],color = colors[i],linestyle = \":\")\n",
    "\n",
    "plt.legend(loc='best', fontsize=14)\n",
    "\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset below contains 20 developmental indices for 132 countries around the world. The country labels (rows) can be found in 'countries.txt,' and the index descriptions can be found in 'indices.txt.' Here we look to explore some of the practical uses of PCA for the purposes of Computer Emulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "developmental_indices = np.loadtxt('dev_indices.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(copy=False)\n",
    "pca = PCA(copy=False, whiten=True, svd_solver='full')\n",
    "Z = pca.fit_transform(scaler.fit_transform(developmental_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Plot the cummulative fraction of variance explained. How many PCs would you recommend using?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Hint: Examine attributes of the object pca, as well as np.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_r = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(F_r)),F_r,'-o')\n",
    "plt.title('Fraction of Variance Explained')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('F_r')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Find the correlation between the first two principal component vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = ??\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Plot the second principal component against the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(??, ??)\n",
    "plt.title('Principle Components')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - Bayes Rule with a \"Toy Jet Quenching\" example\n",
    "\n",
    "### Things to practice\n",
    "\n",
    "In this example, we use a very simple model of jet-quenching to practice all the techniques that we have learned above. Given a measurement: yexp +/- ystat +/- ysys, and a model M : parameters --> predictions,\n",
    "\n",
    "1) Make a parameter design on which we run the computer model\n",
    "\n",
    "2) Use Principal Component Analysis (PCA) to do dimension reduction\n",
    "\n",
    "3) Build Gaussian Process emulators (GP) and train on the design\n",
    "\n",
    "4) Construct proir, likelihood and posterior function\n",
    "\n",
    "5) Use MCMC to marginalize the posterior distribution\n",
    "\n",
    "6) Analyse the posterior distribution of the parameters\n",
    "\n",
    "### A toy-model for jet quenching\n",
    "The energy loss $\\Delta E$ of a particle with energy $E$ follows a $\\Gamma$-distribution,\n",
    "\\begin{equation}\n",
    "P(\\Delta E) = \\Delta E ^ {\\left(\\mu/\\sigma\\right)^2-1} e^{-\\mu\\Delta E/\\sigma^2}\n",
    "\\end{equation}\n",
    "$\\mu$ and $\\sigma$ are the mean and std of the energy loss distribution, parametrized as\n",
    "\\begin{eqnarray}\n",
    "\\mu &=& A \\sqrt{E},\\\\\n",
    "\\sigma &=& B\\mu.\n",
    "\\end{eqnarray}\n",
    "Therefore, this model has two parameters $A, B$. We assume that\n",
    "\n",
    "1) The model is perfect (which is often not the case).\n",
    "\n",
    "2) The true values are $A=1$ and $B=0.5$.\n",
    "\n",
    "### Observables and measurements\n",
    "\n",
    "The observable is an analog of $R_{AA}$. Given a reference spectrum,\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{dN_0}{dp_T} = \\frac{p_T}{\\left(3^2 + p_T^2\\right)^3}\n",
    "\\end{eqnarray}\n",
    "\n",
    "And the $R_{AA}$ is calcualted as the ratio between the quenched and reference spectra,\n",
    "\n",
    "\\begin{eqnarray}\n",
    "R_{AA} = \\frac{dN_1/dp_T}{dN_0/dp_T} =  \\frac{\\int d\\delta p_T P(\\delta p_T) \\frac{dN_0}{dp_T}(p_T+\\delta p_T)}{dN_0/dp_T}\n",
    "\\end{eqnarray}\n",
    "\n",
    "For constructing the measurements, we consider two types of uncertainty: uncorrelated statisitcal errors and correlated systematic errors. Although the correlation among uncertainties is not one of the major topics of this example, we will see at the very end that how different treatments of the correlation affect the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad\n",
    "from scipy.special import gammaln\n",
    "\n",
    "# In this example, we use a very simple model of jet-quenching to practice all the\n",
    "# all the techniques we learned above:\n",
    "# Given a measurement: yexp +/- ystat +/- ysys,\n",
    "#       and a model M : parameters --> predictions\n",
    "# 1) Make a parameter design on which we run the computer model\n",
    "# 2) Use Principal Component Analysis (PCA) to do dimension reduction\n",
    "# 3) Build Gaussian Process emulators (GP) and train on the design\n",
    "# 4) Construct proir, likelihood and posterior function\n",
    "# 5) Use MCMC to marginalize the posterior distribution\n",
    "# 6) Analyse the posterior distribution of the parameters\n",
    "\n",
    "\n",
    "########### A simple model that calculates a \"R_AA\" ########################\n",
    "# Baseline of particle production: dN0/dpT ~ pT/(3^2 + pT^2)^3\n",
    "@np.vectorize\n",
    "def dN0_dpT(pT):\n",
    "    return pT/(3.**2 + pT**2)**3\n",
    "\n",
    "# The spectrum after energy loss:\n",
    "@np.vectorize\n",
    "def dN1_dpT(pT, A, B):\n",
    "    # dP: The probability of a particle with pT to loose delta_pT\n",
    "    # Assume: \n",
    "    #     1) delta_pT follows a Gamma Distribution\n",
    "    #     2) mean-pT-loss: <delta_pT> = A*sqrt(pT)\n",
    "    #     3) pT-loss-fluctuation <delta_pT^2> - <delta_pT>^2 = B*<delta_pT>\n",
    "    # The two parameters A and B are to be extracted from \"data\"\n",
    "    def dP(delta_pT, pT, A, B):\n",
    "        mean = A*pT**0.5\n",
    "        std = B*mean\n",
    "        alpha = mean**2/std**2\n",
    "        beta = mean/std**2\n",
    "        x = beta*delta_pT\n",
    "        return np.exp( (alpha-1.)*np.log(x) - gammaln(alpha) - x ) * beta\n",
    "\n",
    "    # The spectrum after energy loss is a convolution of dN0/dpT and dP\n",
    "    # dN1/dpT = [integral] dN0/dpT(pT+Delta_pT) * dP(Delta_pT) * d[Delta_pT]\n",
    "    def f(ln_1_delta_pT, pT, A, B):\n",
    "        delta_pT = np.exp(ln_1_delta_pT) - 1.\n",
    "        return dN0_dpT(pT+delta_pT) * dP(delta_pT, pT+delta_pT, A, B) * (1.+delta_pT)\n",
    "    result, _, = quad(f, 0.0, np.log(1+5*pT), args=(pT, A, B))\n",
    "    return result\n",
    "\n",
    "# \"Experimental data\"\n",
    "# Assume the model is perfect, and the truth values of A and B are 1.0 and 0.5\n",
    "truth = [1., 0.5]\n",
    "# The Measurement measure the truth Raa, subject to limited statistics and systematic bias\n",
    "# yexp = y_true + ystat + ysys\n",
    "# Default: 10% relative statistical uncertainty, ystat=0\n",
    "@np.vectorize\n",
    "def Measurement(StatLevel, SysLevel):\n",
    "    np.random.seed(10)\n",
    "    pTbin = np.array([1,2,3,4,5,6,10,15,20,30,40,60,100])\n",
    "    pT = (pTbin[1:] + pTbin[:-1])/2.\n",
    "    be = (pTbin[1:] - pTbin[:-1])/2.\n",
    "    y_truth = dN1_dpT(pT, truth[0], truth[1])/dN0_dpT(pT)\n",
    "    results = y_truth * np.random.normal(1.0, StatLevel, pT.shape[0]) \\\n",
    "              * np.random.normal(1.0, SysLevel)\n",
    "    return pT, be, y_truth, results, StatLevel*results, SysLevel*y_truth\n",
    "\n",
    "# The model without known the true value of A and B\n",
    "# With what we have learned, the probablity distribution of A, B\n",
    "# will be inferred from data using this model (a perfect model).\n",
    "@np.vectorize\n",
    "def Model(pT, A, B):\n",
    "    return dN1_dpT(pT, A, B)/dN0_dpT(pT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################Step 1: Get the Measurement########################\n",
    "#  Choose the magnitudes of Stat and Sys error, (recommanded 10%)\n",
    "#  1) plot the true Raa v.s. pT\n",
    "#  2) plot the experimental data with stat and sys error\n",
    "#\n",
    "###################################################################\n",
    "\n",
    "pT, pTbin, ytruth, yexp, ystat, ysys =\\\n",
    "    Measurement(StatLevel=.05, SysLevel=.1)\n",
    "    \n",
    "# plot True Raa here\n",
    "plt.plot(??, ??, 'k.') \n",
    "\n",
    "# plot Raa measurement with stat errorbars\n",
    "plt.errorbar(x=??, xerr=??, y=??, yerr=??, fmt='rD') \n",
    "\n",
    "# plot sys errorband, y1:lowerbounds, y2: higherbounds\n",
    "plt.fill_between(x=pT, y1=yexp-??, y2=yexp+??, color='r', alpha=.3)\n",
    "\n",
    "plt.ylim(0,1)\n",
    "plt.semilogx()\n",
    "plt.xlabel(r'$p_T$ [GeV]', fontsize=15)\n",
    "plt.ylabel(r'$R_{AA}$', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################Step 2: ###################################\n",
    "# Make a design over the parameter space (A, B)\n",
    "# 1) What is a reasonable range of the prior? \n",
    "# ( A and B cannot be too close to 0 due to numerics )\n",
    "# 2) Generate the design and rescale it to the desired range\n",
    "#    Hint: linear rescale x from (0,1) to y from (a,b):\n",
    "#           y = (1-x)*a + x*b\n",
    "# 3) Run model() on each design points\n",
    "#    The design matrix model_data should have a shape: \n",
    "#           N_design x N_pT\n",
    "rangeA = [??, ??]\n",
    "rangeB = [??, ??]\n",
    "ranges = np.array([rangeA, rangeB])\n",
    "hypercube = generate_lhs(npoints=??, ndim=??, seed=80)\n",
    "design = ??????????????? # rescale the hypercube to desired range\n",
    "# Run model on design points\n",
    "model_data = ???????????????????\n",
    "\n",
    "\n",
    "# plot design points\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(design.T[0], design.T[1])\n",
    "plt.xlabel(\"$A$\", fontsize=15)\n",
    "plt.ylabel(\"$B$\", fontsize=15)\n",
    "# plot all calulations\n",
    "plt.subplot(1,2,2)\n",
    "for y in model_data:\n",
    "    plt.plot(pT, y, 'r-', alpha=0.3)\n",
    "    plt.ylim(0,1)\n",
    "plt.semilogx()\n",
    "plt.xlabel(r'$p_T$ [GeV]', fontsize=15)\n",
    "plt.ylabel(r'$R_{AA}$', fontsize=15)\n",
    "plt.tight_layout(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Step 3: apply PCA ######################################\n",
    "# We don't need a separate GP for each pT point\n",
    "# 1) Try keeping different number of principal components (npc).\n",
    "#    How many pc(s) do you think is enought for this exercises?\n",
    "# 2) Take a look at the feature of each pc (figure 3).\n",
    "#    What each pc does in terms of decomposing the data?\n",
    "# 3) Look at the coorelation between PC1 and PC2 (figure 2),\n",
    "#    are they completely uncorrelated? Combine with your\n",
    "#    observations from (figure 3), can you explain what\n",
    "#    causes the correlation?\n",
    "################################################################\n",
    "npc = ??\n",
    "scaler = StandardScaler(copy=True)\n",
    "pca = PCA(copy=True, whiten=True, svd_solver='full')\n",
    "\n",
    "# Keep only the first `npc` principal components\n",
    "Z = pca.fit_transform(scaler.fit_transform(model_data))[:,:npc]\n",
    "\n",
    "# The transformation matrix from PC to Physical space\n",
    "Trans_Matrix = (  pca.components_\n",
    "                * np.sqrt(pca.explained_variance_[:, np.newaxis])\n",
    "                * scaler.scale_)\n",
    "\n",
    "# Estimate the covariance of the negelected PCs\n",
    "Residual_Cov = np.dot(Trans_Matrix[npc:].T, Trans_Matrix[npc:])\n",
    "\n",
    "# plotting\n",
    "plt.figure(figsize=(15,4))\n",
    "F_r = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(range(len(F_r)), F_r, '-o')\n",
    "plt.title('Fraction of Variance Explained')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('F_r')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(Z[:, 0], Z[:, 1])\n",
    "plt.xlabel('$Z_0$', fontsize=15)\n",
    "plt.ylabel('$Z_1$', fontsize=15)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "for comp, color in zip(pca.components_, 'rgb'):\n",
    "    plt.plot(pT, comp, color=color)\n",
    "plt.xlabel('$p_T$', fontsize=15)\n",
    "plt.ylabel('$Features$', fontsize=15)\n",
    "plt.tight_layout(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Step 4-1: Building Emulators #############################\n",
    "# Using an Exp-Squared kernel + a white kernel (accounting \n",
    "# for numerical error of model calculations)\n",
    "# 1) Put in initial length scales for param A and B\n",
    "# 2) Put in reasonable lenght scales bounds for optimization\n",
    "# 3) Fit separate emulatior to each principal component.\n",
    "#    Take a look at the optimized hyper-parameters.\n",
    "#    What do they mean?\n",
    "kernel = (\n",
    "    1. * kernels.RBF(\n",
    "        length_scale=[??, ??],\n",
    "        length_scale_bounds=[(??, ??), (??, ??)]\n",
    "    )  \n",
    "    + kernels.WhiteKernel(.1)\n",
    ")\n",
    "\n",
    "# Build and train each GP\n",
    "gps = [ GPR(kernel=kernel, n_restarts_optimizer=10) \n",
    "        for i in range(npc) ]\n",
    "for i, gp in enumerate(gps):\n",
    "    gp.fit(??, ??)\n",
    "    print('RBF: ', gp.kernel_.get_params()['k1'])\n",
    "    print('White: ', gp.kernel_.get_params()['k2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 4-2: Validating the emulators #######################\n",
    "# It is important to validate the performance of emulators to\n",
    "# make sure they behave as expected.\n",
    "# 1) Pick 6 random combinations of A and B. Compare the\n",
    "#    emulators prediction and the model calculations.\n",
    "#    Do they agree? \n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True)\n",
    "\n",
    "for (a, b) in [(.4, .22), (??, ??), (??, ??), \n",
    "               (??, ??),  (??, ??), (??, ??)]:\n",
    "    # GP prediction\n",
    "    z = np.array([gp.predict([(a, b)])[0] for gp in gps])\n",
    "    pred = np.dot(z, Trans_Matrix[:z.shape[-1]])\n",
    "    pred += scaler.mean_\n",
    "    \n",
    "    # model calcuatlion\n",
    "    calc = Model(pT, a, b)\n",
    "    \n",
    "    ax1.plot(pT, calc, 'ro', alpha=0.7)\n",
    "    ax1.plot(pT, pred, 'b--', alpha=0.7)\n",
    "    ax2.plot(pT, (pred-calc)/calc, 'b--', alpha=0.7)\n",
    "\n",
    "ax1.semilogx()\n",
    "ax1.set_xlabel(r'$p_T$ [GeV]', fontsize=15)\n",
    "ax1.set_ylabel(r'$R_{AA}$', fontsize=15)\n",
    "ax2.set_ylim(-.2, .2)\n",
    "ax2.set_xlabel(r'$p_T$ [GeV]', fontsize=15)\n",
    "ax2.set_ylabel('relative error', fontsize=15)\n",
    "\n",
    "plt.tight_layout(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Helper functions for this block ###################\n",
    "from scipy.linalg import lapack\n",
    "# calculate the log of Gaussian density with \n",
    "# residual dy = y-mu and covariance matrix cov.\n",
    "# - 1/2 * dy^T * cov^[-1] * dy - 1/2*ln(|cov|)\n",
    "def lnLL(dy, cov):\n",
    "    L, info = lapack.dpotrf(cov, clean=False)\n",
    "    alpha, info = lapack.dpotrs(L, dy)\n",
    "    return -.5*np.dot(dy, alpha)-np.log(L.diagonal()).sum()\n",
    "\n",
    "# Transform a covariance matrix from the PC space \n",
    "# back to the physical space\n",
    "def transform_cov(std):\n",
    "    cov = np.matmul(Trans_Matrix[:npc].T*std**2, \n",
    "                    Trans_Matrix[:npc])\\\n",
    "        + Residual_Cov \n",
    "    return cov\n",
    "\n",
    "####### Step 5: Construct the posterior #################\n",
    "# Remember that from Bayes' Theorem:\n",
    "#      Posterior  = prior * likelihood\n",
    "# and:\n",
    "#      ln(Posterior) = ln(prior) + ln(likelihood)\n",
    "# and:\n",
    "#      theta = [A, B]\n",
    "# 1) Complete the returns of the prior \"prior_ln_pdf(theta)\"\n",
    "# 2) The sys-error is correlated, while the stat one is not\n",
    "#    We provide two types of covariance matrixx\n",
    "#        2.1) cov_exp1 treats sys-error as uncorrelated\n",
    "#        2.2) cov_exp2 treats sys-error as correlated\n",
    "#    Start with 2.1) and later try 2.2) to see the effects\n",
    "#    on the posterior distribution of A and B.\n",
    "# 3) Complete the likelihood_ln_pdf(theta) function\n",
    "def prior_ln_pdf(theta):\n",
    "    if (theta<ranges[:,0]).any() or (theta>ranges[:,1]).any():\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "# Pick your experimental covariance matrix\n",
    "Assume_SysError_Corr = ?? # try True or False\n",
    "cov_exp = np.diag(ystat**2) + np.outer(ysys, ysys) \\\n",
    "          if Assume_SysError_Corr else \\\n",
    "          np.diag(ystat**2) + np.diag(ysys**2)\n",
    "\n",
    "def likelihood_ln_pdf(theta):\n",
    "    z, stdz = np.array([gp.predict([theta], return_std=True) for gp in gps]).T[0]\n",
    "    pred = np.dot(z, Trans_Matrix[:z.shape[-1]])\n",
    "    pred += scaler.mean_\n",
    "    cov_emulator = transform_cov(std=stdz)\n",
    "    dy = ??\n",
    "    cov = ??\n",
    "    return lnLL(dy, cov)\n",
    "\n",
    "# Finally ln(Posterior) = ln(prior) + ln(likelihood)\n",
    "def posterior_ln_pdf(theta):\n",
    "    ln_pr = prior_ln_pdf(theta)\n",
    "    ln_like = likelihood_ln_pdf(theta) \n",
    "    return ln_pr + ln_like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Step 6: Run MCMC ###########################\n",
    "# Fill 1) the number of samples 2) burnin steps\n",
    "# 3) dimsional of the problem 4) number of mcmc walkers\n",
    "# Hint: this may take a while, so start with smaller numbers\n",
    "nsteps = ??\n",
    "nburnin = ??\n",
    "ndim = ??\n",
    "nwalkers = 20\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, \n",
    "                                posterior_ln_pdf)\n",
    "p0 = np.random.rand(nwalkers, ndim)\n",
    "p0 = (1.-p0)*ranges[:, 0] +  p0*ranges[:, 1]\n",
    "out_post = sampler.run_mcmc(p0, nsteps)\n",
    "samples = sampler.chain[:, nburnin:, :].reshape((-1, ndim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 7: Analyze the posterior distribution ########\n",
    "# 1) Run this block and plot the posterior distribution\n",
    "# 2) Does the posterior fairly estimates the true values (red)?\n",
    "# 3) How does the posterior change it we take into account the\n",
    "#    correlation among the sys-error?\n",
    "figure, axes = plt.subplots(figsize=(5,5), \n",
    "                            ncols=ndim, nrows=ndim)\n",
    "names = [r\"$A$\", r\"$B$\"]\n",
    "for i, row in enumerate(axes):\n",
    "    for j, ax in enumerate(row):\n",
    "        if i==j:\n",
    "            ax.hist(samples[:,i], bins=40,\n",
    "                    range=ranges[i], histtype='step', \n",
    "                    normed=True)\n",
    "            ax.set_xlabel(names[i])\n",
    "            ax.axvline(x=truth[i], color='r', linewidth=1)\n",
    "            ax.set_xlim(*ranges[j])\n",
    "        if i>j:\n",
    "            ax.hist2d(samples[:, j], samples[:, i], \n",
    "                      bins=40, range=[ranges[j], ranges[i]], \n",
    "                      cmap='Blues')\n",
    "            ax.set_xlabel(names[j])\n",
    "            ax.set_ylabel(names[i])\n",
    "            ax.axvline(x=truth[j], color='r', linewidth=1)\n",
    "            ax.axhline(y=truth[i], color='r', linewidth=1)\n",
    "            ax.plot(truth[j], truth[i], 'ro')\n",
    "            ax.set_xlim(*ranges[j])\n",
    "            ax.set_ylim(*ranges[i])\n",
    "        if i<j:\n",
    "            ax.axis('off')\n",
    "plt.tight_layout(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting observables\n",
    "param_samples = samples[ np.random.choice(range(len(samples)),50), :]\n",
    "z  = np.array([gp.predict(param_samples) for gp in gps]).T\n",
    "pred = np.dot(z, Trans_Matrix[:z.shape[-1]])\n",
    "pred += scaler.mean_\n",
    "for i, y in enumerate(pred):\n",
    "    plt.plot(pT, y, 'b-', alpha=0.15, label=\"Posterior\" if i==0 else '')\n",
    "plt.errorbar(pT, yexp, yerr=ystat, xerr=pTbin, fmt='ro', label=\"Measurements\")\n",
    "for xl, xr, yl, yh in zip(pT-pTbin, pT+pTbin, yexp-ysys, yexp+ysys):\n",
    "    plt.fill_between([xl,xr],[yl, yl], [yh, yh], facecolor='None', edgecolor='r')\n",
    "plt.ylim(0,1)\n",
    "plt.semilogx()\n",
    "plt.xlabel(r'$p_T$ [GeV]', fontsize=15)\n",
    "plt.ylabel(r'$R_{AA}$', fontsize=15)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
